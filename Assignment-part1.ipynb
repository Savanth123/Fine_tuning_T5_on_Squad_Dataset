{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4519f12c-2798-4092-b25b-25acc4a15f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0897172a-1086-416c-8783-3015a8c29664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task-1 : Loaded google/flan-t5-small Pretrained Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71083254-9e01-4ea8-94fd-c13a589de675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e224ee-889d-41f2-8d58-9e3afd85845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task-2: Text Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b785a2df-c34b-4609-9fa1-e24adba495bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[21603,    10,  1447,  1286,     3,     9,    97,     6,     3,     9,\n",
      "         17766,   141,     3,     9, 29669,    24,  7245,     3,     9,  7069,\n",
      "          6182,   334,   239,     5,    37, 17766,   261,    12,  1789,    24,\n",
      "          6182,    11,  3807,   631,   540,    12,   942,    70,   384,    31,\n",
      "             7,   239,    18,   235,    18,  1135,   523,     5,   555,   239,\n",
      "             6,     8, 17766,   816,    24,     3,    99,     3,    88,   228,\n",
      "           129,    72,   224,  7069,  5875,    11,   143,     3,     9,   418,\n",
      "            13,   540,    11,   582,     3,     9, 18407,   568,     5,    37,\n",
      "         17766,  1500,    12,  1340,     8, 29669,    11,  2036,    66,     8,\n",
      "          7069,  5875,    45,   165,  9883,     5,   282,  1116,    38,    79,\n",
      "          4792,     8,  5963,    11,  2946,     8, 29669,    22,     7,  9883,\n",
      "             6,    79,   435,   150,  5875,     5,    37, 27539, 17766,  5723,\n",
      "            79,   141, 10932,    70,   336,  3487,    91,    13, 30337,     5,\n",
      "             1]], device='cuda:0') <class 'torch.Tensor'>\n",
      "Original Text:\n",
      " \n",
      "Once upon a time, a farmer had a goose that laid a golden egg every day. The farmer used to sell that egg \n",
      "and earn enough money to meet their family's day-to-day needs. One day, the farmer thought that if he could \n",
      "get more such golden eggs and make a lot of money and become a wealthy person. The farmer decided to cut the \n",
      "goose and remove all the golden eggs from its stomach. As soon as they killed the bird and opened the goose’s \n",
      "stomach, they found no eggs. The foolish farmer realized they had destroyed their last resource out of greed.\n",
      "\n",
      "\n",
      "\n",
      "Generated Summary:\n",
      " The farmer had a goose that laid a golden egg every day. The farmer decided to cut the goose and remove all the golden eggs from its stomach. The farmer realized they had destroyed their last resource out of greed.\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "input_text = \"\"\"\n",
    "Once upon a time, a farmer had a goose that laid a golden egg every day. The farmer used to sell that egg \n",
    "and earn enough money to meet their family's day-to-day needs. One day, the farmer thought that if he could \n",
    "get more such golden eggs and make a lot of money and become a wealthy person. The farmer decided to cut the \n",
    "goose and remove all the golden eggs from its stomach. As soon as they killed the bird and opened the goose’s \n",
    "stomach, they found no eggs. The foolish farmer realized they had destroyed their last resource out of greed.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and generate summary\n",
    "inputs = tokenizer(\"summarize: \" + input_text, return_tensors=\"pt\", max_length=2048, truncation=True).input_ids.to(\"cuda\")\n",
    "print(inputs, type(inputs))\n",
    "summary_ids = model.generate(inputs, max_length = 1024)\n",
    "\n",
    "# Decode and print the summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"Original Text:\\n\", input_text)\n",
    "print(\"\\nGenerated Summary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d39b207-31c4-4420-a72d-c0c2184af04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task-3 Q/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd94a15a-9c09-4533-a749-85b0bbfe0c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\savan\\OneDrive - Lineaje\\Desktop\\NLP\\lib\\site-packages\\transformers\\generation\\utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Anil agarwal announced how much investment ?\n",
      "Q&A Output: <pad> <unk> 1.54 lakh crore</s>\n"
     ]
    }
   ],
   "source": [
    "context_qa = \"\"\" Vedanta's chairman Anil Agarwal earlier this week announced the biggest investment of ₹1.54 lakh crore for setting \n",
    "up the country's first-ever semiconductor chip plant in Gujarat. This led to a strong buying on stock exchanges that drove Vedanta \n",
    "to rise nearly 18% this week. However, on the last trading day of the current week, Vedanta shares pulled back and slipped by at least \n",
    "nearly 9% on BSE after the company said, the semiconductor plant project is not under their ambit but will be undertaken by Volcan Investments.\n",
    "\"\"\"\n",
    "question_qa = \"Anil agarwal announced how much investment ?\"\n",
    "input_text_qa = f\"question: {question_qa} context: {context_qa}\"\n",
    "input_ids_qa = tokenizer(input_text_qa, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs_qa = model.generate(input_ids_qa)\n",
    "print(\"Question:\", question_qa)\n",
    "print(\"Q&A Output:\", tokenizer.decode(outputs_qa[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64c7949e-39ee-4bdf-b07b-d1ef9d445cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task-4 Translate from english to french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "066b75d6-ef8a-4c5f-a8e4-dd3a9210d9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>Quels sont les mesures à la station de train et à la station\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Translate the following English text to French: What way is it to the museum/train station? \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da5daa3c-c7b7-4141-8660-62898df59df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task-5 Printing model layers and dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2cb2723-45d9-4bd1-97f8-16fefc04e0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name: shared.weight, Dimensions: torch.Size([32128, 512])\n",
      "Layer Name: encoder.block.0.layer.0.SelfAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.0.layer.0.SelfAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.0.layer.0.SelfAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.0.layer.0.SelfAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Dimensions: torch.Size([32, 6])\n",
      "Layer Name: encoder.block.0.layer.0.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: encoder.block.0.layer.1.DenseReluDense.wi_0.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.0.layer.1.DenseReluDense.wi_1.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.0.layer.1.DenseReluDense.wo.weight, Dimensions: torch.Size([512, 1024])\n",
      "Layer Name: encoder.block.0.layer.1.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: encoder.block.1.layer.0.SelfAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.1.layer.0.SelfAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.1.layer.0.SelfAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.1.layer.0.SelfAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: encoder.block.1.layer.0.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: encoder.block.1.layer.1.DenseReluDense.wi_0.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.1.layer.1.DenseReluDense.wi_1.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.1.layer.1.DenseReluDense.wo.weight, Dimensions: torch.Size([512, 1024])\n",
      "Layer Name: encoder.block.1.layer.1.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: encoder.block.2.layer.0.SelfAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.2.layer.0.SelfAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.2.layer.0.SelfAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.2.layer.0.SelfAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: encoder.block.2.layer.0.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: encoder.block.2.layer.1.DenseReluDense.wi_0.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.2.layer.1.DenseReluDense.wi_1.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.2.layer.1.DenseReluDense.wo.weight, Dimensions: torch.Size([512, 1024])\n",
      "Layer Name: encoder.block.2.layer.1.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: encoder.block.3.layer.0.SelfAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.3.layer.0.SelfAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.3.layer.0.SelfAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.3.layer.0.SelfAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: encoder.block.3.layer.0.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: encoder.block.3.layer.1.DenseReluDense.wi_0.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.3.layer.1.DenseReluDense.wi_1.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.3.layer.1.DenseReluDense.wo.weight, Dimensions: torch.Size([512, 1024])\n",
      "Layer Name: encoder.block.3.layer.1.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: encoder.block.4.layer.0.SelfAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.4.layer.0.SelfAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.4.layer.0.SelfAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.4.layer.0.SelfAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: encoder.block.4.layer.0.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: encoder.block.4.layer.1.DenseReluDense.wi_0.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.4.layer.1.DenseReluDense.wi_1.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.4.layer.1.DenseReluDense.wo.weight, Dimensions: torch.Size([512, 1024])\n",
      "Layer Name: encoder.block.4.layer.1.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: encoder.block.5.layer.0.SelfAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.5.layer.0.SelfAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.5.layer.0.SelfAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.5.layer.0.SelfAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: encoder.block.5.layer.0.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: encoder.block.5.layer.1.DenseReluDense.wi_0.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.5.layer.1.DenseReluDense.wi_1.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.5.layer.1.DenseReluDense.wo.weight, Dimensions: torch.Size([512, 1024])\n",
      "Layer Name: encoder.block.5.layer.1.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: encoder.block.6.layer.0.SelfAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.6.layer.0.SelfAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.6.layer.0.SelfAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.6.layer.0.SelfAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: encoder.block.6.layer.0.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: encoder.block.6.layer.1.DenseReluDense.wi_0.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.6.layer.1.DenseReluDense.wi_1.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.6.layer.1.DenseReluDense.wo.weight, Dimensions: torch.Size([512, 1024])\n",
      "Layer Name: encoder.block.6.layer.1.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: encoder.block.7.layer.0.SelfAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.7.layer.0.SelfAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.7.layer.0.SelfAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.7.layer.0.SelfAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: encoder.block.7.layer.0.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: encoder.block.7.layer.1.DenseReluDense.wi_0.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.7.layer.1.DenseReluDense.wi_1.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.7.layer.1.DenseReluDense.wo.weight, Dimensions: torch.Size([512, 1024])\n",
      "Layer Name: encoder.block.7.layer.1.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: encoder.final_layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.0.layer.0.SelfAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.0.layer.0.SelfAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.0.layer.0.SelfAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.0.layer.0.SelfAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Dimensions: torch.Size([32, 6])\n",
      "Layer Name: decoder.block.0.layer.0.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.0.layer.1.EncDecAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.0.layer.1.EncDecAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.0.layer.1.EncDecAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.0.layer.1.EncDecAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.0.layer.1.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.0.layer.2.DenseReluDense.wi_0.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.0.layer.2.DenseReluDense.wi_1.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.0.layer.2.DenseReluDense.wo.weight, Dimensions: torch.Size([512, 1024])\n",
      "Layer Name: decoder.block.0.layer.2.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.1.layer.0.SelfAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.1.layer.0.SelfAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.1.layer.0.SelfAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.1.layer.0.SelfAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.1.layer.0.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.1.layer.1.EncDecAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.1.layer.1.EncDecAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.1.layer.1.EncDecAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.1.layer.1.EncDecAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.1.layer.1.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.1.layer.2.DenseReluDense.wi_0.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.1.layer.2.DenseReluDense.wi_1.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.1.layer.2.DenseReluDense.wo.weight, Dimensions: torch.Size([512, 1024])\n",
      "Layer Name: decoder.block.1.layer.2.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.2.layer.0.SelfAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.2.layer.0.SelfAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.2.layer.0.SelfAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.2.layer.0.SelfAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.2.layer.0.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.2.layer.1.EncDecAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.2.layer.1.EncDecAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.2.layer.1.EncDecAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.2.layer.1.EncDecAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.2.layer.1.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.2.layer.2.DenseReluDense.wi_0.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.2.layer.2.DenseReluDense.wi_1.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.2.layer.2.DenseReluDense.wo.weight, Dimensions: torch.Size([512, 1024])\n",
      "Layer Name: decoder.block.2.layer.2.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.3.layer.0.SelfAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.3.layer.0.SelfAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.3.layer.0.SelfAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.3.layer.0.SelfAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.3.layer.0.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.3.layer.1.EncDecAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.3.layer.1.EncDecAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.3.layer.1.EncDecAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.3.layer.1.EncDecAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.3.layer.1.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.3.layer.2.DenseReluDense.wi_0.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.3.layer.2.DenseReluDense.wi_1.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.3.layer.2.DenseReluDense.wo.weight, Dimensions: torch.Size([512, 1024])\n",
      "Layer Name: decoder.block.3.layer.2.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.4.layer.0.SelfAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.4.layer.0.SelfAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.4.layer.0.SelfAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.4.layer.0.SelfAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.4.layer.0.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.4.layer.1.EncDecAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.4.layer.1.EncDecAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.4.layer.1.EncDecAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.4.layer.1.EncDecAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.4.layer.1.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.4.layer.2.DenseReluDense.wi_0.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.4.layer.2.DenseReluDense.wi_1.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.4.layer.2.DenseReluDense.wo.weight, Dimensions: torch.Size([512, 1024])\n",
      "Layer Name: decoder.block.4.layer.2.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.5.layer.0.SelfAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.5.layer.0.SelfAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.5.layer.0.SelfAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.5.layer.0.SelfAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.5.layer.0.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.5.layer.1.EncDecAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.5.layer.1.EncDecAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.5.layer.1.EncDecAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.5.layer.1.EncDecAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.5.layer.1.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.5.layer.2.DenseReluDense.wi_0.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.5.layer.2.DenseReluDense.wi_1.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.5.layer.2.DenseReluDense.wo.weight, Dimensions: torch.Size([512, 1024])\n",
      "Layer Name: decoder.block.5.layer.2.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.6.layer.0.SelfAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.6.layer.0.SelfAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.6.layer.0.SelfAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.6.layer.0.SelfAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.6.layer.0.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.6.layer.1.EncDecAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.6.layer.1.EncDecAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.6.layer.1.EncDecAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.6.layer.1.EncDecAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.6.layer.1.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.6.layer.2.DenseReluDense.wi_0.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.6.layer.2.DenseReluDense.wi_1.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.6.layer.2.DenseReluDense.wo.weight, Dimensions: torch.Size([512, 1024])\n",
      "Layer Name: decoder.block.6.layer.2.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.7.layer.0.SelfAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.7.layer.0.SelfAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.7.layer.0.SelfAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.7.layer.0.SelfAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.7.layer.0.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.7.layer.1.EncDecAttention.q.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.7.layer.1.EncDecAttention.k.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.7.layer.1.EncDecAttention.v.weight, Dimensions: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.7.layer.1.EncDecAttention.o.weight, Dimensions: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.7.layer.1.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.block.7.layer.2.DenseReluDense.wi_0.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.7.layer.2.DenseReluDense.wi_1.weight, Dimensions: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.7.layer.2.DenseReluDense.wo.weight, Dimensions: torch.Size([512, 1024])\n",
      "Layer Name: decoder.block.7.layer.2.layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: decoder.final_layer_norm.weight, Dimensions: torch.Size([512])\n",
      "Layer Name: lm_head.weight, Dimensions: torch.Size([32128, 512])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer Name: {name}, Dimensions: {param.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea624d19-904b-41ba-9922-613c293f8311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task - 6: Printing total number of parameters/weights in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb90c1ed-def2-4e39-bf8c-052c4e07ae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters in the Model: 76961152\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters in the Model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32095829-488f-44f8-b4a9-b4caf3f02c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-7: Setting the tensor in the final layer to all zeros - decoder.final_layer_norm.weight,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da536a20-abf8-4909-8695-78b04f1cc378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.decoder.final_layer_norm.weight.data.fill_(0.0)\n",
    "model.decoder.final_layer_norm.weight.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dbcc606-25f1-40dd-aaa5-41f16b440592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-8: Verifying Q&A task after resetting weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c064b652-68af-433e-ba58-8c392ba5a1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A Output After Reset: <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "outputs_qa_after_reset = model.generate(input_ids_qa)\n",
    "print(\"Q&A Output After Reset:\", tokenizer.decode(outputs_qa_after_reset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7eb7672a-07ea-43ea-92c7-31dd17ea7b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task- 9: Replace the decoder.final_layer_norm.weight with a layer of smaller dimensions (256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "303e341d-6a2e-48b9-b956-cb5f42fbb0bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "#new_dim = 256  and the default is 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa4f4f1-3c2c-4d2b-ab11-32f743d43534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the final layer normalization weight with a layer of smaller dimensions which is 256\n",
    "new_final_layer_norm_size = model.decoder.final_layer_norm.weight.data.shape[0] // 2\n",
    "new_final_layer_norm = torch.nn.LayerNorm(new_final_layer_norm_size, device=model.device)\n",
    "model.decoder.final_layer_norm = new_final_layer_norm\n",
    "\n",
    "# Adjusting dependent layers diemnsions in the decoder\n",
    "for name, param in model.decoder.named_parameters():\n",
    "    print(name)\n",
    "    if \"final_layer_norm\" in name:\n",
    "        continue\n",
    "\n",
    "    if \"SelfAttention\" in name or \"EncDecAttention\" in name or \"DenseReluDense\" in name:\n",
    "        param.data = param.data[:, :new_final_layer_norm_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960be405-4a1b-4643-bb6c-e5c7ef6f498e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
